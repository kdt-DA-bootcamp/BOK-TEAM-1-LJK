import asyncio
import json
import os
from playwright.async_api import async_playwright

# 2018ë…„ 1ì›” ~ 2025ë…„ 2ì›”
YEARS = list(range(2018,2026))
MONTHS = list(range(1, 3))

# ë™ì‹œ í¬ë¡¤ë§ ì œí•œ (í•œ ë²ˆì— 5ê°œì”©)
MAX_CONCURRENT_CRAWLS = 5

# ë°ì´í„° ì €ì¥ í´ë” ì„¤ì •
OUTPUT_DIR = "mk_news_data"
os.makedirs(OUTPUT_DIR, exist_ok=True)

async def fetch_article_urls(year, month):
    start_date = f"{year}-{str(month).zfill(2)}-01"
    end_date = f"{year}-{str(month).zfill(2)}-31"
    search_url = f"https://www.mk.co.kr/search/news?word=ê¸ˆë¦¬&sort=asc&dateType=direct&startDate={start_date}&endDate={end_date}&searchField=all&includeWord=ê¸ˆë¦¬&newsType=all"

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)  
        # headless ëª¨ë“œë¡œ ì‹¤í–‰ (ë¹ ë¥´ê²Œ í¬ë¡¤ë§)
        context = await browser.new_context()
        page = await context.new_page()

        print(f"\nğŸ” [STEP 1] {year}-{month} ê²€ìƒ‰ í˜ì´ì§€ ì—´ê¸°: {search_url}")
        await page.goto(search_url)
        await page.wait_for_load_state("domcontentloaded")

        # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­
        while True:
            try:
                more_button = await page.query_selector("button[data-btn-more][data-append-selector='#list_area']")
                if more_button:
                    print(f"ğŸ”„ {year}-{month}: 'ë”ë³´ê¸°' ë²„íŠ¼ í´ë¦­")
                    await more_button.click()
                    await asyncio.sleep(3)  
                    # í´ë¦­ í›„ ë¡œë”© ëŒ€ê¸°
                    await page.wait_for_load_state("domcontentloaded")
                else:
                    print(f"{year}-{month}: 'ë”ë³´ê¸°' ë²„íŠ¼ ì—†ìŒ. URL ìˆ˜ì§‘ ì™„ë£Œ!")
                    break
            except Exception as e:
                print(f"{year}-{month}: 'ë”ë³´ê¸°' ë²„íŠ¼ í´ë¦­ ì˜¤ë¥˜: {e}")
                break

        # ê¸°ì‚¬ URL í¬ë¡¤ë§
        articles = await page.query_selector_all("ul.news_list li.news_node a")
        article_urls = list(set([await article.get_attribute("href") for article in articles]))

        print(f"\n[{year}-{month}] ì´ {len(article_urls)}ê°œì˜ ê¸°ì‚¬ URL ë°œê²¬!")

        await browser.close()
        return article_urls


async def scrape_article(semaphore, context, url):
    """ê¸°ì‚¬ ë³¸ë¬¸ì„ í¬ë¡¤ë§ (ì œëª©ì— 'ìŠ¤íƒë¡ 'ì´ í¬í•¨ëœ ê¸°ì‚¬ëŠ” ì œì™¸)"""
    async with semaphore:  # ë™ì‹œ í¬ë¡¤ë§ ê°œìˆ˜ ì œí•œ
        page = await context.new_page()
        print(f"\nğŸ” [START] ê¸°ì‚¬ í¬ë¡¤ë§: {url}")

        try:
            await page.goto(url, timeout=15000)
            await page.wait_for_load_state("domcontentloaded")

            # ì œëª© í¬ë¡¤ë§
            title_element = await page.query_selector("h2.news_ttl")
            title = await title_element.text_content() if title_element else "ì œëª© ì—†ìŒ"

            # ì œëª© í•„í„°ë§ (ìŠ¤íƒë¡  í¬í•¨ ê¸°ì‚¬ ì œì™¸)
            if "ìŠ¤íƒë¡ " in title:
                print(f"âš ï¸ [ì œì™¸] ì œëª© '{title}' (ìŠ¤íƒë¡  í¬í•¨ ê¸°ì‚¬)")
                await page.close()
                return None

            # ë‚ ì§œ í¬ë¡¤ë§
            date_element = await page.query_selector("div.time_area dl dd")
            date = await date_element.text_content() if date_element else "ë‚ ì§œ ì—†ìŒ"

            # ë³¸ë¬¸ í¬ë¡¤ë§
            try:
                content = await page.inner_text("div.news_cnt_detail_wrap", timeout=5000)
            except:
                content = "ë³¸ë¬¸ ì—†ìŒ"

            print(f"ğŸ“° ì œëª©: {title}")
            print(f"ğŸ—“ ë‚ ì§œ: {date}")
            print(f"ğŸ“ ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {content[:200]}...")

        except Exception as e:
            print(f"[ERROR] {url} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None

        await page.close()
        return {"title": title, "date": date, "content": content, "url": url}


async def process_month(year, month):
    """í•œ ë‹¬ ë‹¨ìœ„ë¡œ ê¸°ì‚¬ URLì„ ìˆ˜ì§‘í•˜ê³  í¬ë¡¤ë§ (ë¸Œë¼ìš°ì € ìœ ì§€ & ë™ì‹œ í¬ë¡¤ë§ ì œí•œ)"""
    article_urls = await fetch_article_urls(year, month)
    if not article_urls:
        print(f"[{year}-{month}] í¬ë¡¤ë§í•  ê¸°ì‚¬ ì—†ìŒ. ê±´ë„ˆëœ€!")
        return

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)  
        # ì†ë„ ìµœì í™”
        context = await browser.new_context()
        semaphore = asyncio.Semaphore(MAX_CONCURRENT_CRAWLS)  # ë™ì‹œ í¬ë¡¤ë§ ê°œìˆ˜ ì œí•œ

        # ê¸°ì‚¬ í¬ë¡¤ë§ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰ (ìµœëŒ€ ë™ì‹œ 5ê°œ)
        tasks = [scrape_article(semaphore, context, url) for url in article_urls]
        articles_data = await asyncio.gather(*tasks)

        await browser.close()

    # "ìŠ¤íƒë¡ "ì´ í¬í•¨ëœ ê¸°ì‚¬ ì œì™¸ í›„ JSON ì €ì¥
    filtered_articles = [article for article in articles_data if article]

    json_filename = f"{OUTPUT_DIR}/mk_news_{year}_{str(month).zfill(2)}.json"
    with open(json_filename, "w", encoding="utf-8") as f:
        json.dump(filtered_articles, f, ensure_ascii=False, indent=4)

    print(f"\nğŸ‰ [{year}-{month}] í¬ë¡¤ë§ ì™„ë£Œ! (ì´ {len(filtered_articles)}ê°œ ê¸°ì‚¬)")
    print(f"ğŸ’¾ '{json_filename}' ì €ì¥ ì™„ë£Œ!")


async def main():
    """ì „ì²´ ê¸°ê°„ì„ í•œ ë‹¬ì”© ë‚˜ëˆ„ì–´ í¬ë¡¤ë§"""
    for year in YEARS:
        for month in MONTHS:
            print(f"\nğŸš€ {year}-{month} í¬ë¡¤ë§ ì‹œì‘...")
            await process_month(year, month)
            await asyncio.sleep(2)  # í¬ë¡¤ë§ ê°„ê²© ì¶”ê°€ (ì„œë²„ ë¶€ë‹´ ë°©ì§€)

asyncio.run(main())
